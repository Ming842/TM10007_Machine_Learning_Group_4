{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template -- ECG data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E528Yh1i4RWg"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to use from Colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "from tqdm import tqdm  # Progress bar\n",
        "from scipy.stats import uniform, loguniform\n",
        "\n",
        "# Extract dataset\n",
        "with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tm10007_ml/ecg')\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of features: {len(data.columns) - 1}')  # Excluding label column\n",
        "\n",
        "# Extract features and labels\n",
        "X = data.iloc[:, :-1].values  # All columns except the last one\n",
        "y = data.iloc[:, -1].values   # Last column as labels\n",
        "\n",
        "# Scale the features\n",
        "scaler = RobustScaler(quantile_range=(30, 70))\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensionality while preserving 99% of variance\n",
        "pca = PCA(n_components=0.99)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f'Reduced number of features after PCA: {X_pca.shape[1]}')\n",
        "\n",
        "# Split into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter search space for RBF kernel\n",
        "param_distributions = {\n",
        "    'C': loguniform(0.01, 500),  # Log-uniform distribution for better coverage of large ranges\n",
        "    'gamma': loguniform(0.0001, 10),  # Log-uniform for better granularity\n",
        "    'class_weight': ['balanced', None],  # Adjusts for class imbalance\n",
        "    'shrinking': [True, False],  # Use shrinking heuristic or not\n",
        "    'tol': uniform(1e-5, 1e-2),  # Tolerance for stopping\n",
        "}\n",
        "\n",
        "\n",
        "# Number of iterations for RandomizedSearchCV\n",
        "n_iter_search = 500  # Increased to explore more combinations\n",
        "\n",
        "# Initialize SVM model with RBF kernel\n",
        "svm = SVC(kernel='rbf')\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=svm,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=n_iter_search,  # Large number of trials\n",
        "    scoring='accuracy',\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Timing the process\n",
        "start_time = time.time()\n",
        "\n",
        "with tqdm(total=n_iter_search, desc=\"Running Hyperparameter Optimization\") as pbar:\n",
        "    random_search.fit(X_train, y_train)\n",
        "    pbar.close()\n",
        "\n",
        "elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
        "\n",
        "# Get best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nBest hyperparameters found:\", best_params)\n",
        "print(f\"Best cross-validation accuracy: {random_search.best_score_:.4f}\")\n",
        "print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "xai9DpmN5agD",
        "outputId": "1a0e8d99-e086-486b-a99f-62d06f67b82b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n",
            "The number of samples: 827\n",
            "The number of features: 9000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}