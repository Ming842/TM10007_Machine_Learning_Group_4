{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template -- ECG data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E528Yh1i4RWg"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to use from Colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Extract dataset\n",
        "with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tm10007_ml/ecg')\n",
        "\n",
        "# Extract dataset\n",
        "data = pd.read_csv('ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of features: {len(data.columns) - 1}')  # Excluding label column\n",
        "\n",
        "# Extract features and labels\n",
        "X = data.iloc[:, :-1].values  # All columns except the last one\n",
        "y = data.iloc[:, -1].values   # Last column as labels\n",
        "\n",
        "# Scale the features\n",
        "scaler = RobustScaler(quantile_range=(30,70))\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensionality while preserving 95% of variance\n",
        "pca = PCA(n_components=0.99)  # Retain 95% of the variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "# scaler_pca = StandardScaler()\n",
        "# X_pca = scaler_pca.fit_transform(X_pca)  # Normalize after PCA\n",
        "\n",
        "print(f'Reduced number of features after PCA: {X_pca.shape[1]}')\n",
        "\n",
        "print(\"Before PCA: Mean & Variance\")\n",
        "print(np.mean(X_scaled, axis=0)[:10])  # Print first 10 features\n",
        "print(np.var(X_scaled, axis=0)[:10])\n",
        "\n",
        "print(\"\\nAfter PCA: Mean & Variance\")\n",
        "print(np.mean(X_pca, axis=0)[:10])\n",
        "print(np.var(X_pca, axis=0)[:10])\n",
        "\n",
        "\n",
        "# Split into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Randomized Search\n",
        "param_distributions = {\n",
        "    'C': uniform(0.1, 10),  # Regularization parameter\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Kernel coefficient for ‘rbf’\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Different kernel types\n",
        "}\n",
        "\n",
        "# Initialize SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=svm,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of different hyperparameter combinations to try\n",
        "    scoring='accuracy',\n",
        "    cv=5,  # 5-fold cross-validation on training data\n",
        "    n_jobs=-1,  # Use all available cores\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit Randomized Search on training data (80% of X_train used for training, 20% for validation)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and best score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best cross-validation accuracy: \", random_search.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Test set accuracy: \", test_accuracy)\n"
      ],
      "metadata": {
        "id": "xai9DpmN5agD",
        "outputId": "1a0e8d99-e086-486b-a99f-62d06f67b82b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n",
            "The number of samples: 827\n",
            "The number of features: 9000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}