{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3544e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
    "from scipy.stats import uniform, loguniform\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
    "\n",
    "# Load dataset\n",
    "data_path = os.path.join(os.getcwd(), 'ecg_data.csv')\n",
    "data = pd.read_csv(data_path, index_col=0)\n",
    "\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of features: {len(data.columns) - 1}')  # Excluding label column\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.iloc[:, :-1].values  # All columns except the last one\n",
    "y = data.iloc[:, -1].values   # Last column as labels\n",
    "\n",
    "# Scale the features\n",
    "scaler = RobustScaler(quantile_range=(25, 75))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define outer and inner loop random seeds\n",
    "outer = range(0, 5)  # Outer loop (train-test variations)\n",
    "inner = range(0, 5)  # Inner loop (hyperparameter tuning)\n",
    "\n",
    "best_params_list = {}  # Stores best params per (outer, inner) iteration\n",
    "hyperparam_accuracies = {}  # Stores accuracies per hyperparameter set\n",
    "\n",
    "# Generate a fixed set of random hyperparameter sets (same for all outer loops)\n",
    "fixed_hyperparam_sets = []\n",
    "for _ in range(10):  # Generate 10 random hyperparameter sets\n",
    "    param_set = {\n",
    "        'C': loguniform(0.01, 1000).rvs(),\n",
    "        'gamma': loguniform(0.0001, 100).rvs(),\n",
    "        'class_weight': random.choice(['balanced', None]),\n",
    "        'shrinking': random.choice([True, False]),\n",
    "        'tol': uniform(1e-5, 1e-2).rvs()\n",
    "    }\n",
    "    fixed_hyperparam_sets.append(param_set)\n",
    "    hyperparam_accuracies[str(param_set)] = []  # Initialize list for accuracy tracking\n",
    "\n",
    "# Outer loop for train-test split variations\n",
    "for outer_rand in tqdm(outer, desc='Outer Loop'):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=outer_rand, stratify=y  # Maintain class proportions\n",
    "    )\n",
    "\n",
    "    # Apply PCA to reduce dimensionality (fit only on training data)\n",
    "    pca = PCA(n_components=0.99)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Apply SMOTE to balance the training set\n",
    "    smote = SMOTE(random_state=42)  # Initialize SMOTE\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_pca, y_train)  # Apply SMOTE only to training data\n",
    "\n",
    "    print(f\"Explained Variance by PCA Components: {pca.explained_variance_ratio_.sum()}\")\n",
    "    print(f\"Resampled training set size: {len(y_train_resampled)}\")\n",
    "\n",
    "    # Inner loop for hyperparameter tuning\n",
    "    for inner_rand, param_set in tqdm(enumerate(fixed_hyperparam_sets), desc='Inner Loop', leave=True):\n",
    "        X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(\n",
    "            X_train_resampled, y_train_resampled, test_size=0.15, random_state=inner_rand, stratify=y_train_resampled  # Maintain class proportions\n",
    "        )\n",
    "\n",
    "        # Initialize SVM model with RBF kernel\n",
    "        svm = SVC(kernel='rbf', **param_set, probability=True)  # Enable probability for AUC calculation\n",
    "\n",
    "        # Train and evaluate model\n",
    "        start_time = time.time()\n",
    "        svm.fit(X_train_train, y_train_train)\n",
    "        elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "\n",
    "        # Best model evaluation\n",
    "        y_pred = svm.predict(X_test_pca)  # Evaluate on the untouched test set\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Compute confusion matrix for sensitivity and specificity\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()  # True negatives, False positives, False negatives, True positives\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "\n",
    "        # Compute AUC\n",
    "        auc = roc_auc_score(y_test, svm.predict_proba(X_test_pca)[:, 1])\n",
    "\n",
    "        # Store results\n",
    "        param_key = str(param_set)  # Convert to string for dict key\n",
    "        hyperparam_accuracies[param_key].append(accuracy)  # Store accuracy\n",
    "\n",
    "        best_params_list[f\"Outer {outer_rand} - Inner {inner_rand}\"] = param_set.copy()\n",
    "        best_params_list[f\"Outer {outer_rand} - Inner {inner_rand}\"]['accuracy'] = accuracy\n",
    "        best_params_list[f\"Outer {outer_rand} - Inner {inner_rand}\"]['sensitivity'] = sensitivity\n",
    "        best_params_list[f\"Outer {outer_rand} - Inner {inner_rand}\"]['specificity'] = specificity\n",
    "        best_params_list[f\"Outer {outer_rand} - Inner {inner_rand}\"]['auc'] = auc\n",
    "        best_params_list[f\"Outer {outer_rand} - Inner {inner_rand}\"]['f1-score'] = f1\n",
    "\n",
    "        print(f\"\\nOuter {outer_rand}, Inner {inner_rand} -> Hyperparameters: {param_set}\")\n",
    "        print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Select the best classifier based on the F1 score\n",
    "best_f1_score = -1\n",
    "best_classifier = None\n",
    "best_metrics = {}\n",
    "\n",
    "for key, val in best_params_list.items():\n",
    "    f1 = val['f1-score']\n",
    "    \n",
    "    # Update if current F1 score is higher\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_classifier = key\n",
    "        best_metrics = val  # Store the metrics of the best classifier\n",
    "\n",
    "# Print the best classifier based on F1 score\n",
    "print(f\"\\nüèÜ Best Classifier Based on F1 Score: {best_classifier}\")\n",
    "print(f\"F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "# Print the detailed metrics of the best classifier\n",
    "print(f\"\\nDetailed Metrics of the Best Classifier:\")\n",
    "print(f\"Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"Sensitivity: {best_metrics['sensitivity']:.4f}\")\n",
    "print(f\"Specificity: {best_metrics['specificity']:.4f}\")\n",
    "print(f\"AUC: {best_metrics['auc']:.4f}\")\n",
    "print(f\"F1 Score: {best_metrics['f1-score']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
